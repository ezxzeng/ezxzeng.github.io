
@inproceedings{zeng2024decoding,
  title     = {SCALEX: Scalable Concept and Latent Exploration for Diffusion Models},
  author    = {Zeng, E Zhixuan and Chen, Yuhao and Wong, Alexander},
  booktitle = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2026},
  month     = {Mar.},
  doi       = {10.48550/arXiv:2511.13750},
  pdf       = {https://arxiv.org/pdf/2511.13750},
  abstract  = {Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns.
We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.},
  selected  = True,
  preview   = {food_tsne.png}
}

@inproceedings{chen2022metagraspnet,
  title     = {MetaGraspNet: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis},
  author    = {Chen, Yuhao and Gilles, Maximilian and Zeng, E Zhixuan and Wong, Alexander},
  booktitle = {2022 IEEE CASE},
  year      = {2022},
  preview   = {drop_item.jpg},
  award     = {Finalist},
  abstract  = {Autonomous bin picking poses significant challenges to vision-driven robotic systems given the complexity of the problem, ranging from various sensor modalities, to highly entangled object layouts, to diverse item properties and gripper types. Existing methods often address the problem from one perspective. Diverse items and complex bin scenes require diverse picking strategies together with advanced reasoning. As such, to build robust and effective machine-learning algorithms for solving this complex task requires significant amounts of comprehensive and high quality data. Collecting such data in real world would be too expensive and time prohibitive and therefore intractable from a scalability perspective. To tackle this big, diverse data problem, we take inspiration from the recent rise in the concept of metaverses, and introduce MetaGraspNet, a large-scale photo-realistic bin picking dataset constructed via physics-based metaverse synthesis. The proposed dataset contains 217k RGBD images across 82 different article types, with full annotations for object detection, amodal perception, keypoint detection, manipulation order and ambidextrous grasp labels for a parallel-jaw and vacuum gripper. We also provide a real dataset consisting of over 2.3k fully annotated high-quality RGBD images, divided into 5 levels of difficulties and an unseen object set to evaluate different object and layout properties. Finally, we conduct extensive experiments showing that our proposed vacuum seal model and synthetic dataset achieves state-of-the-art performance and generalizes to real world use-cases.},
  pdf       = {https://arxiv.org/pdf/2208.03963},
  selected  = True
}

@article{zeng2024covid,
  title     = {COVID-Net L2C-ULTRA: An Explainable Linear-Convex Ultrasound Augmentation Learning Framework to Improve COVID-19 Assessment and Monitoring},
  author    = {Zeng, E Zhixuan and Ebadi, Ashkan and Florea, Adrian and Wong, Alexander},
  journal   = {Sensors},
  volume    = {24},
  number    = {5},
  pages     = {1664},
  year      = {2024},
  publisher = {MDPI},
  preview   = {warp_example_base.jpg},
  abstract  = {While no longer a public health emergency of international concern, COVID-19 remains an established and ongoing global health threat. As the global population continues to face significant negative impacts of the pandemic, there has been an increased usage of point-of-care ultrasound (POCUS) imaging as a low-cost, portable, and effective modality of choice in the COVID-19 clinical workflow. A major barrier to the widespread adoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert clinicians who can interpret POCUS examinations, leading to considerable interest in artificial intelligence-driven clinical decision support systems to tackle this challenge. A major challenge to building deep neural networks for COVID-19 screening using POCUS is the heterogeneity in the types of probes used to capture ultrasound images (e.g., convex vs. linear probes), which can lead to very different visual appearances. In this study, we propose an analytic framework for COVID-19 assessment able to consume ultrasound images captured by linear and convex probes. We analyze the impact of leveraging extended linear-convex ultrasound augmentation learning on producing enhanced deep neural networks for COVID-19 assessment, where we conduct data augmentation on convex probe data alongside linear probe data that have been transformed to better resemble convex probe data. The proposed explainable framework, called COVID-Net L2C-ULTRA, employs an efficient deep columnar anti-aliased convolutional neural network designed via a machine-driven design exploration strategy. Our experimental results confirm that the proposed extended linear–convex ultrasound augmentation learning significantly increases performance, with a gain of 3.9% in test accuracy and 3.2% in AUC, 10.9% in recall, and 4.4% in precision. The proposed method also demonstrates a much more effective utilization of linear probe images through a 5.1% performance improvement in recall when such images are added to the training dataset, while all other methods show a decrease in recall when trained on the combined linear–convex dataset. We further verify the validity of the model by assessing what the network considers to be the critical regions of an image with our contribution clinician.},
  pdf       = {https://www.mdpi.com/1424-8220/24/5/1664}
}

@inproceedings{zeng2022keypoints,
  author    = {Zeng, E Zhixuan and Chen, Yuhao and Wong, Alexander},
  booktitle = {Journal of Computational Vision and Imaging Systems},
  title     = {Investigating Use of Keypoints for Object Pose Recognition},
  year      = {2022},
  abstract  = {Object pose detection is a task that is highly useful for a variety of object manipulation tasks such as robotic grasping and tool handling. Perspective-n-Point matching between keypoints on the objects offers a way to perform  pose estimation where the keypoints also provide inherent object information, such as corner locations and object part sections, without the need to reference a separate 3D model. Existing works focus on scenes with little occlusion and limited object categories. In this study, we demonstrate the feasibility of a pose estimation network based on detecting semantically important keypoints on the MetagraspNet dataset which contains heavy occlusion and greater scene complexity. We further discuss various challenges in using semantically important keypoints as a way to perform object pose estimation.  These challenges include maintaining consistent keypoint definition, as well as dealing with heavy occlusion and similar visual features. },
  pdf       = {https://openjournals.uwaterloo.ca/index.php/vsl/article/view/5382}
}

@inproceedings{zeng2023shapeshift,
  title     = {ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping},
  author    = {Zeng, E Zhixuan and Chen, Yuhao and Wong, Alexander},
  booktitle = {WICV workshop},
  maintitle = {CVPR 2023},
  year      = {2023},
  preview   = {shapeshift.jpg},
  abstract  = {Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object's pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.},
  pdf       = {https://arxiv.org/pdf/2304.04861},
  selected  = True
}

@inproceedings{zeng2023explaining_xai,
  title     = {Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability},
  author    = {Zeng*, E Zhixuan and Gunraj*, Hayden and Fernandez, Sheldon and Wong, Alexander},
  booktitle = {XAI4CV workshop},
  maintitle = {CVPR 2023},
  year      = {2023},
  preview   = {https://i.imgur.com/4sfJMv9.png},
  abstract  = {Explainability plays a crucial role in providing a more comprehensive understanding of deep learning models' behaviour. This allows for thorough validation of the model's performance, ensuring that its decisions are based on relevant visual indicators and not biased toward irrelevant patterns existing in training data. However, existing methods provide only instance-level explainability, which requires manual analysis of each sample. Such manual review is time-consuming and prone to human biases. To address this issue, the concept of second-order explainable AI (SOXAI) was recently proposed to extend explainable AI (XAI) from the instance level to the dataset level. SOXAI automates the analysis of the connections between quantitative explanations and dataset biases by identifying prevalent concepts. In this work, we explore the use of this higher-level interpretation of a deep neural network's behaviour to allows us to "explain the explainability" for actionable insights. Specifically, we demonstrate for the first time, via example classification and segmentation cases, that eliminating irrelevant concepts from the training set based on actionable insights from SOXAI can enhance a model's performance.},
  pdf       = {https://arxiv.org/pdf/2306.08780},
  selected  = True
}

@inproceedings{chen2023mmrnet,
  title     = {MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy},
  author    = {Chen, Yuhao and Gunraj, Hayden and Zeng, E Zhixuan and Meyer, Robbie and Gilles, Maximilian and Wong, Alexander},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {68--77},
  year      = {2023},
  preview   = {mmrnet.png},
  abstract  = {Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.},
  pdf       = {https://arxiv.org/pdf/2210.10842},
  selected  = True
}

@article{zeng2024understanding,
  title    = {Understanding the Limitations of Diffusion Concept Algebra Through Food},
  author   = {Zeng, E Zhixuan and Chen, Yuhao and Wong, Alexander},
  journal  = {arXiv preprint arXiv:2406.03582},
  year     = {2024},
  preview  = {chicken_burger.png},
  abstract = {Image generation techniques, particularly latent diffusion models, have exploded in popularity in recent years. Many techniques have been developed to manipulate and clarify the semantic concepts these large-scale models learn, offering crucial insights into biases and concept relationships. However, these techniques are often only validated in conventional realms of human or animal faces and artistic style transitions. The food domain offers unique challenges through complex compositions and regional biases, which can shed light on the limitations and opportunities within existing methods. Through the lens of food imagery, we analyze both qualitative and quantitative patterns within a concept traversal technique. We reveal measurable insights into the model's ability to capture and represent the nuances of culinary diversity, while also identifying areas where the model's biases and limitations emerge.},
  pdf      = {https://arxiv.org/pdf/2406.03582}
}

@article{Zeng_Zeng_2024,
  title    = {Beyond the Scoreboard: Advancing Fairness in Athlete Selection with Simulation-Based Tournament Strategies},
  volume   = {9},
  url      = {https://openjournals.uwaterloo.ca/index.php/vsl/article/view/5867},
  doi      = {10.15353/jcvis.v9i1.10015},
  number   = {1},
  journal  = {Journal of Computational Vision and Imaging Systems},
  author   = {Zeng, E. Zhixuan and Zeng, Yuhong},
  year     = {2024},
  month    = {Apr.},
  pages    = {58–61},
  abstract = {The process of selecting athletes for competitive sports teams is often undermined by the limitations of traditional tournament formats, which can misrepresent the true skill levels of participants. This issue is exemplified by a scenario observed in a table tennis team tryout, where a moderately skilled player advanced to the final round due to consistently facing weaker opponents, while more adept players were eliminated early against stronger competitors. Such occurrences cast doubt on the fairness and effectiveness of single elimination tournaments for player assessment. Addressing these concerns, our study conducts a thorough analysis of various tournament selection strategies, including single elimination, Swiss tournaments, and novel graph and sorting-based methods. By modeling players as Gaussian distributions with established mean skill levels, we simulate match outcomes to quantitatively evaluate the efficiency and accuracy of each strategy. Our evaluation employs two loss functions: Strict Loss, to gauge ranking precision, and Binary Loss, to assess the accuracy in identifying top performers. The experimental results reveal significant insights. Strategies integrating Elo ratings with circular graph approaches show enhanced performance, particularly in larger player groups, while TrueSkill and single elimination exhibit limitations in scalability and nuanced player ranking. The Swiss tournament, although consistent, experiences fluctuations in loss, suggesting areas for refinement. Notably, a novel graph-based strategy emerges as a stable and efficient alternative, underscoring its potential for future research. These findings aim to guide the development of more equitable and precise selection processes in sports team composition.},
  pdf      = {https://openjournals.uwaterloo.ca/index.php/vsl/article/view/5867}
}
